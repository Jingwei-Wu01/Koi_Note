## COMET: Commonsense Transformers for Automatic Knowledge Graph Construction

**作者将常识获取作为知识库的构建，并研究大规模语言模型是否能够有效地学习生成自动构建常识知识库（KB）所需的知识。**

主要创新点是:

1. 提出了一种基于生成模型的**知识图谱自动构建**方法。

2. 使用大规模transformer语言模型进行预训练,将语言表示迁移到显式常识知识的生成中。

   > Dddd

3. 在ATOMIC和ConceptNet两个常识知识库上验证了方法的有效性。

   > COMET论文中,作者在ATOMIC和ConceptNet两个常识知识库上验证了所提出方法的有效性,主要验证了以下两点:
   >
   > 1. 生成知识的<u>质量</u>
   >    - 在ATOMIC数据集上,作者进行了人工评估,让**人工判断COMET生成的知识三元组是否合理有效**。结果显示,77.5%的生成结果被人工判断为正确的。
   >    - 在ConceptNet数据集上,作者使用一个**预训练的评分模型**来自动判断生成的知识质量。结果显示,95.25%的生成结果可以被模型判断为正确。
   > 2. 生成知识的<u>新颖性</u>
   >    - 在ATOMIC数据集上,作者统计了生成结果中完全新的知识三元组和新的宾语短语o的比例。结果显示,COMET可以生成新的节点和边扩展知识图谱。
   >    - 在ConceptNet数据集上,作者计算了生成结果中不出现在训练数据中的知识三元组和新词o的比例。结果也表明COMET可以生成新的知识。
   >    - 作者还分析了生成结果与训练数据的编辑距离,证明许多生成知识在词序列上有明显不同,不是简单的训练知识的重复。
   >
   > 总之,实验结果显示COMET可以生成质量高且新颖的常识知识,既可以形成新的知识节点,也可以产生新的知识边,实现了知识图谱的扩展。



?? 然而，常识知识并不能干净地融入到一个模式中，**将两个具有已知关系的实体进行比较，导致目前的方法将 "实体 "建模为自然语言短语，将关系建模为任何能将它们联系起来的概念**

指标：

1. PPL 语言模型的指标
2. BLEU-2 文本生成
3. N/T



**对于COMET生成新的领域知识的举例,COMET论文中给出了以下例子:**

1. 在ATOMIC数据集上,COMET生成了如下新的因果知识三元组:

- PersonX gives PersonY a pep talk   xAttr helpful

- PersonX pours ___ over PersonY's head  oEffect gets hurt  

- PersonX spoils somebody rotten xIntent to be mean

这些都是模型生成的新**三元组**,可以扩展社交常识领域的知识图谱。

2. 在ConceptNet数据集上,COMET生成了一些新的“is-a”关系知识:

- mango IsA fruit

- doctor CapableOf save life

- bird bone HasProperty fragile

这些新生成的**类别关系知识**可以补充ConceptNet的常识知识库。

所谓领域的知识生成方式指:

COMET模型在ATOMIC等领域种子知识上进行训练,学习到了这种社交/常识领域知识的生成方式,包括:

- 事件情境和对应人的心理状态、反应等之间的因果关系

- 事物类别之间的“is-a”关系

- 事件属性特征之间的“has-property”关系

等。这些都是某一领域知识的典型表示方式。COMET通过学习可以捕捉这些知识生成规律,然后应用到新实例上实现自动生成。

所以COMET通过学习可以获得对特定领域知识的生成方式的建模,然后应用这种生成方式到新情境,实现对该领域知识的扩展。



**COMET是一个自适应框架，通过对语言模型进行<u>种子知识图元组</u>的训练，从语言模型中构建常识知识库。这些元组为COMET提供了必须学习的知识库结构和关系，COMET通过学习来适应从预训练中学习到的语言模型表示，在种子知识图中添加新的节点和边。**